{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"Creating a custom text normalization transformer\"\n",
        "\n",
        "\"Text normalization reduces the number of dimensions, decreasing sparsity. Besides the simple filtering of tokens (removing punctuation and stopwords),\"\n",
        "\"there are two primary methods for text normalization: stemming and lemmatization.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ms2i-2Goinvu",
        "outputId": "05da6267-dc1a-4278-ee0e-c477a08a8574"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'there are two primary methods for text normalization: stemming and lemmatization.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Yda2gcVGHIKd"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "   \n",
        "   def __init__(self, language='english'):\n",
        "       self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
        "       self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        " \n",
        "   def is_punct(self, token):\n",
        "      return all(\n",
        "           unicodedata.category(char).startswith('P') for char in token)\n",
        "\n",
        "\n",
        "   def is_stopword(self, token):\n",
        "      return token.lower() in self.stopwords\n",
        "\n",
        "#checks if every character in the token has a Unicode category that starts with 'P' (for punctuation);"
      ],
      "metadata": {
        "id": "tEYHbnCphdXs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "     def normalize(self, document):\n",
        "        return [\n",
        "         self.lemmatize(token, tag).lower()\n",
        "         for paragraph in document\n",
        "         for sentence in paragraph\n",
        "         for (token, tag) in sentence\n",
        "         if not self.is_punct(token) and not self.is_stopword(token)]\n",
        "\n",
        "#This method applies the filtering functions to remove unwanted tokens and then lemmatizes them."
      ],
      "metadata": {
        "id": "DZTCE9oRhyiI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(self, token, pos_tag):\n",
        "     tag = {\n",
        "     'N': wn.NOUN,\n",
        "     'V': wn.VERB,\n",
        "     'R': wn.ADV,\n",
        "     'J': wn.ADJ\n",
        "     }.get(pos_tag[0], wn.NOUN)\n",
        "     return self.lemmatizer.lemmatize(token, tag)\n",
        "\n",
        "#The lemmatize() method first converts the Penn\n",
        "#Treebank part-of-speech tags that are the default tag set in the\n",
        "#nltk.pos_tag function to WordNet tags, selecting nouns by default."
      ],
      "metadata": {
        "id": "2GoxkeHYiBcC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, documents):\n",
        "    for document in documents:\n",
        "      yield self.normalize(document)\n",
        "\n",
        "#Finally, we must add the Transformer interface, allowing us to add this\n",
        "#class to a Scikit-Learn pipeline"
      ],
      "metadata": {
        "id": "Rs0l7EuiiR4K"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}